{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DrAlexSanz/nlpv2-course/blob/master/Markov_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0u6P4ESmE2Kz"
      },
      "source": [
        "## Text generation\n",
        "\n",
        "Markov models are typical first order models (word depends only on previous word). Let's use a second order model. Words depend on the previous TWO words.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import string\n",
        "np.random.seed(1234)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "initial = {} # Start of a phrase, initial word distribution\n",
        "first_order = {} # Second word only\n",
        "second_order = {} # second order transition probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "def remove_punkt(s):\n",
        "    return s.translate(str.maketrans(\"\", \"\", string.punctuation))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "def add2dict(dictionary, key, value):\n",
        "    \"\"\"\n",
        "    keys are tuples. The previous two words. Values are the probabilities.\n",
        "    \"\"\"\n",
        "    if key not in dictionary:\n",
        "        dictionary[key] = []\n",
        "    dictionary[key].append(value)\n",
        "\n",
        "    return\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "for line in open(\"robert_frost.txt\"):\n",
        "    tokens = remove_punkt(line.rstrip().lower()).split()\n",
        "    T = len(tokens)\n",
        "    for i in range(T):\n",
        "        t = tokens[i]\n",
        "        if i == 0:\n",
        "            # Distribution of first word\n",
        "            initial[t] = initial.get(t, 0.) + 1\n",
        "        else:\n",
        "            t_1 = tokens[i-1]\n",
        "            if i == T-1: # The one before the last word --> probability of ending hte line\n",
        "                add2dict(second_order, (t_1, t), \"END\")\n",
        "            \n",
        "            # Not elif, two independent checks\n",
        "            if i == 1: # First order, distribution of second word given the first word\n",
        "                add2dict(first_order, t_1, t)\n",
        "\n",
        "            else:\n",
        "                t_2 = tokens[i-2]\n",
        "                add2dict(second_order, (t_2, t_1), t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Normalize distributions\n",
        "\n",
        "initial_total = sum(initial.values())\n",
        "\n",
        "for t, c in initial.items():\n",
        "    initial[t] = c/initial_total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "# convert [cat, cat, cat, dog, dog, dog, mouse...]\n",
        "# to {cat: 0.5, dog: 0.4, mouse: 0.1}\n",
        "\n",
        "def list2pdicts(tokens_list):\n",
        "    d = {}\n",
        "    n = len(tokens_list)\n",
        "    \n",
        "    for t in tokens_list: # loop through each token. Each time I see it, I count it.\n",
        "        d[t] = d.get(t, 0.) + 1\n",
        "    \n",
        "    for t, c in d.items(): # Normalize the count from before\n",
        "        d[t] = c/n\n",
        "\n",
        "    return d\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "for t_1, token_list in first_order.items():\n",
        "    first_order[t_1] = list2pdicts(token_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "for k, token_list in second_order.items():\n",
        "    second_order[k] = list2pdicts(token_list) # And this will be a dictionary of dictionaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sample_word(dictionary):\n",
        "    p_0 = np.random.random()\n",
        "    cumulative = 0\n",
        "    for t, p in dictionary.items():\n",
        "        cumulative += p\n",
        "\n",
        "        if p_0 < cumulative:\n",
        "            return t\n",
        "    assert False # Shouldn't reach here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_n_lines(n):\n",
        "    \"\"\" Generate n sentences, 1 sentence at a time\"\"\"\n",
        "    for i in range(n):\n",
        "        sentence = []\n",
        "        # Initial word\n",
        "        w_0 = sample_word(initial)\n",
        "        sentence.append(w_0)\n",
        "\n",
        "        # Second word\n",
        "        w_1 = sample_word(first_order[w_0])\n",
        "        sentence.append(w_1)\n",
        "\n",
        "        # Second order transitions until END\n",
        "\n",
        "        while True:\n",
        "            w_2 = sample_word(second_order[(w_0, w_1)])\n",
        "            if w_2 == \"END\":\n",
        "                break\n",
        "            sentence.append(w_2)\n",
        "            w_0 = w_1\n",
        "            w_1 = w_2\n",
        "        print(\" \".join(sentence))\n",
        "\n",
        "    return\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "he gained no foothold but pursued\n",
            "does the rain seem to say\n",
            "me im not blaming him\n",
            "with our arms at the level of our united strengths to do right\n",
            "heres hoping she gets her drink and be one traveler long i lay\n",
            "and when ive said why shouldnt they be married\n",
            "with all this now too much to keep accounts\n",
            "dont ask for better\n",
            "i guess estelle and i can\n",
            "should say our stock was petered out\n",
            "hes been in a window\n",
            "too lofty and original to rage\n"
          ]
        }
      ],
      "source": [
        "generate_n_lines(12)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "22920"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(initial) ** 3\n",
        "\n",
        "dict_1_elem = len(initial)\n",
        "dict_2_elem = 0\n",
        "for elem in first_order.items():\n",
        "    dict_2_elem += len(elem)\n",
        "dict_2_elem\n",
        "dict_3_elem = 0\n",
        "for elem in second_order.items():\n",
        "    for elem2 in elem:\n",
        "        dict_3_elem += len(elem2)\n",
        "dict_3_elem"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOQSLauyM8jbiKuEewwqL2p",
      "include_colab_link": true,
      "name": "Markov models.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "f0fe5aee4ac00dd02374c2a79b5196ede65385b1d7edbd24e38669aaafd896e6"
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 64-bit ('ML_tests': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
